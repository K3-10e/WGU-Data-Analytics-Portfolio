{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4977ea",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<h1>Data Mining - Predictive Analysis</h1>\n",
    "<h2>Part I: Research Question</h2>\n",
    "<h3>A. Purpose of Data Mining Report</h3>\n",
    "<h4>1. Proposal of Question</h4>\n",
    "<p>&nbsp; &nbsp; For this data analysis, I will be using a decision tree model to answer the question\n",
    "“Can a customer’s number of yearly equipment failures be predicted based on their\n",
    "account profile?”</p>\n",
    "<h4>2. Defined Goal</h4>\n",
    "<p>&nbsp; &nbsp; Of all possible variables in the data set, I chose to analyze equipment failure\n",
    "because I want to raise the business’ awareness as to which customers are more\n",
    "susceptible to failing technology. With this information the company can allocate\n",
    "resources to customers who will need it and improve response times and customer\n",
    "satisfaction. Predicting how many customers will require gadget replacements can also\n",
    "help the business configure an annual budget for the devices.</p>\n",
    "\n",
    "<h2>Part II: Method Justification</h2>\n",
    "<h3>B. Justification and Reasoning</h3>\n",
    "<h4>1. Explanation of Prediction</h4>\n",
    "<p>&nbsp; &nbsp; Like a flow chart, a decision tree operates by using nodes that split the data and\n",
    "decide which branch or path a variable will follow. These nodes are selected by picking the\n",
    "‘purest’ (most homogenous values) of the remaining variables as it is the clearest way of\n",
    "sorting data rows. At the end of the tree are the ‘leaves’ where the model determines what\n",
    "the most likely value for the dependent variable is based on its training. All independent\n",
    "variables used to create nodes on the tree are described in section C2 with\n",
    "‘Yearly_equip_failure’ being the dependent variable. From this project I expect to learn the\n",
    "prediction accuracy of using a decision tree with mean squared error, root mean squared\n",
    "error, and R-Squared as measurement metrics, and potentially answer the stated research\n",
    "question above.</p>\n",
    "<h4>2. Summary of Method Assumption</h4>\n",
    "<p>&nbsp; &nbsp; Since the algorithm works by essentially making ‘Yes’ or ‘No’ decisions at every level,\n",
    "it is assumed that variables can be split in a binary way (Ttcbest, 2023). This requires that\n",
    "categorical variables with more than two valid values (such as ‘Gender’) must be converted\n",
    "into dummy variables so the values themselves can be represented as variables with only\n",
    "two possible options, ‘0’ or ‘1’. For numeric variables, the decision tree splits the data\n",
    "through inequalities; for example, the algorithm might pick one branch over another\n",
    "because a customer’s age is greater than 35. Another assumption, as is the case with many\n",
    "other algorithms, is that there are no missing values present in the dataset. The path\n",
    "cannot be pursued down the decision tree if there is no value to analyze, so all cell data\n",
    "must be present.</p>\n",
    "<h4>3. Libraries List</h4>\n",
    "<p>\n",
    "<ul>\n",
    "<li>Pandas - Used to store, manipulate, and read large data sets. Stores ‘churn_clean.csv’\n",
    "as variable in the program</li>\n",
    "<li>Sklearn.model_selection - Provides a feature to split data into testing and training as well as enables\n",
    "hyperparameter tunning with the ‘GridSearchCV()’ function.</li>\n",
    "<li>Sklearn.tree - The decision tree model (DecisionTreeClassifier()) is provided by this library.</li>\n",
    "<li>Sklearn.metrics - Describes the effectiveness of the decision tree model with an accuracy\n",
    "score, mean squared error, root mean squared error, and an R squared score.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<h2>Part III: Data Preparation</h2>\n",
    "<h3>C. Data Preparation Process</h3>\n",
    "<h4>1. Data Preprocessing</h4>\n",
    "<p>&nbsp; &nbsp; As described in B2, categorical variables will need to be converted into dummy\n",
    "variables for analysis. Branches on a decision tree are pursued based on the algorithm’s\n",
    "response to yes-or-no questions, and for some categorical variables that is not a\n",
    "possibility. For example, the variable ‘Gender’ can have the values ‘Female’, ‘Male’, or\n",
    "‘Nonbinary’, which would require a question outside a binary choice to determine the\n",
    "customer’s gender. Converting the variable with the ‘get_dummies()’ function will produce\n",
    "the variables ‘Gender_Female’, ‘Gender_Male’, and ‘Gender_Nonbinary’ with possible\n",
    "values in these columns being ‘0’ or ‘1’. These new columns can then be used in the\n",
    "decision tree’s formulation process since they only have two possible values.</p>\n",
    "<h4>2. Data Set Variables</h4>\n",
    "<p><ul>\n",
    "<li>Area<ul>\n",
    "<li>Data Type: Categorical</li>\n",
    "<li>Description: Area density based on census data. Valid values include ‘Rural’,\n",
    "‘Urban’, ‘Suburban.’</li>\n",
    "</ul></li>\n",
    "<li>Population<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: Population count in a mile radius of the customer, based on\n",
    "census data.</li>\n",
    "</ul></li>\n",
    "<li>Children<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: Number of children a customer has, obtained from sign-up\n",
    "information.</li>\n",
    "</ul></li>\n",
    "<li>Age<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: The customer’s age, obtained from sign-up information.</li>\n",
    "</ul></li>\n",
    "<li>Income<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: The customer’s income, obtained from sign-up information.</li>\n",
    "</ul></li>\n",
    "<li>Gender<ul>\n",
    "<li>Data Type: Categorical</li>\n",
    "<li>Description: The customer's gender, obtained from sign-up information.</li>\n",
    "</ul></li>\n",
    "<li>Outage_sec_perweek<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: Average, in seconds, of outage time in a customer’s\n",
    "neighborhood.</li>\n",
    "</ul></li>\n",
    "<li>Yearly_equip_failure<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: A count of the customer’s annual equipment failure. This is the\n",
    "variable in question for this project.</li>\n",
    "</ul></li>\n",
    "<li>Techie<ul>\n",
    "<li>Data Type: Categorical</li>\n",
    "<li>Description: If the customer considers themselves technologically inclined.</li>\n",
    "</ul></li>\n",
    "<li>InternetService<ul>\n",
    "<li>Data Type: Categorical</li>\n",
    "<li>Description: The internet service provider of the customer.</li>\n",
    "</ul></li>\n",
    "<li>Multiple<ul>\n",
    "<li>Data Type: Categorical</li>\n",
    "<li>Description: If the customer has multiple lines.</li>\n",
    "</ul></li>\n",
    "<li>DeviceProtection<ul>\n",
    "<li>Data Type: Categorical</li>\n",
    "<li>Description: If the customer has the device protection business add-on.</li>\n",
    "</ul></li>\n",
    "<li>TechSupport<ul>\n",
    "<li>Data Type: Categorical</li>\n",
    "<li>Description: If the customer has technical support business add-on.</li>\n",
    "</ul></li>\n",
    "<li>Tenure<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: How long, in months, the customer has been with the business.</li>\n",
    "</ul></li>\n",
    "<li>Bandwidth_GB_Year<ul>\n",
    "<li>Data Type: Numeric</li>\n",
    "<li>Description: Annual data usage of a customer in gigabytes.</li>\n",
    "</ul></li>\n",
    "</ul></p>\n",
    "<h4>3. Steps for Analysis</h4>\n",
    "<p>&nbsp; &nbsp; In preparing data for analysis, data must first be cleaned, then transformed to suit\n",
    "the research’s needs. Discovering and handling duplicates, missing values, and outliers are\n",
    "all parts of the cleaning process that will be applied to ‘churn_clean.csv’ to ensure no\n",
    "errors arise when the testing is conducted. Following that, the data will be reduced to\n",
    "reflect only variables relevant to the analysis and categorical columns will be converted\n",
    "into dummy variables.\n",
    "\n",
    "&nbsp; &nbsp; As an aside, I will not be standardizing or normalizing numeric values as “Decision\n",
    "trees are not sensitive to feature scaling” (Filho, 2023). The algorithm works by making\n",
    "binary decisions so there is virtually no impact when the scale of data is altered.\n",
    "\n",
    "&nbsp; &nbsp; Using the ‘detect_duplicates()’ function, no cell duplicates were detected for the\n",
    "‘CaseOrder’, ‘Customer_id’, or ‘Interaction’ columns, all of which require unique values. By\n",
    "subtracting the length of the database with row duplicates removed from the length of the\n",
    "original database, I can calculate how many row duplicates are present in the dataset.\n",
    "Doing so resulted in no row duplicates detected.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee1dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mDetecting Duplicates\u001b[0m\n",
      "Exact row:  0\n",
      "CaseOrder:  0\n",
      "Customer_id:  0\n",
      "Interaction:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaseOrder</th>\n",
       "      <th>Customer_id</th>\n",
       "      <th>Interaction</th>\n",
       "      <th>UID</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "      <th>...</th>\n",
       "      <th>MonthlyCharge</th>\n",
       "      <th>Bandwidth_GB_Year</th>\n",
       "      <th>Item1</th>\n",
       "      <th>Item2</th>\n",
       "      <th>Item3</th>\n",
       "      <th>Item4</th>\n",
       "      <th>Item5</th>\n",
       "      <th>Item6</th>\n",
       "      <th>Item7</th>\n",
       "      <th>Item8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>K409198</td>\n",
       "      <td>aa90260b-4141-4a24-8e36-b04ce1f4f77b</td>\n",
       "      <td>e885b299883d4f9fb18e39c75155d990</td>\n",
       "      <td>Point Baker</td>\n",
       "      <td>AK</td>\n",
       "      <td>Prince of Wales-Hyder</td>\n",
       "      <td>99927</td>\n",
       "      <td>56.25100</td>\n",
       "      <td>-133.37571</td>\n",
       "      <td>...</td>\n",
       "      <td>172.455519</td>\n",
       "      <td>904.536110</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S120509</td>\n",
       "      <td>fb76459f-c047-4a9d-8af9-e0f7d4ac2524</td>\n",
       "      <td>f2de8bef964785f41a2959829830fb8a</td>\n",
       "      <td>West Branch</td>\n",
       "      <td>MI</td>\n",
       "      <td>Ogemaw</td>\n",
       "      <td>48661</td>\n",
       "      <td>44.32893</td>\n",
       "      <td>-84.24080</td>\n",
       "      <td>...</td>\n",
       "      <td>242.632554</td>\n",
       "      <td>800.982766</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>K191035</td>\n",
       "      <td>344d114c-3736-4be5-98f7-c72c281e2d35</td>\n",
       "      <td>f1784cfa9f6d92ae816197eb175d3c71</td>\n",
       "      <td>Yamhill</td>\n",
       "      <td>OR</td>\n",
       "      <td>Yamhill</td>\n",
       "      <td>97148</td>\n",
       "      <td>45.35589</td>\n",
       "      <td>-123.24657</td>\n",
       "      <td>...</td>\n",
       "      <td>159.947583</td>\n",
       "      <td>2054.706961</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>D90850</td>\n",
       "      <td>abfa2b40-2d43-4994-b15a-989b8c79e311</td>\n",
       "      <td>dc8a365077241bb5cd5ccd305136b05e</td>\n",
       "      <td>Del Mar</td>\n",
       "      <td>CA</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>92014</td>\n",
       "      <td>32.96687</td>\n",
       "      <td>-117.24798</td>\n",
       "      <td>...</td>\n",
       "      <td>119.956840</td>\n",
       "      <td>2164.579412</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>K662701</td>\n",
       "      <td>68a861fd-0d20-4e51-a587-8a90407ee574</td>\n",
       "      <td>aabb64a116e83fdc4befc1fbab1663f9</td>\n",
       "      <td>Needville</td>\n",
       "      <td>TX</td>\n",
       "      <td>Fort Bend</td>\n",
       "      <td>77461</td>\n",
       "      <td>29.38012</td>\n",
       "      <td>-95.80673</td>\n",
       "      <td>...</td>\n",
       "      <td>149.948316</td>\n",
       "      <td>271.493436</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>M324793</td>\n",
       "      <td>45deb5a2-ae04-4518-bf0b-c82db8dbe4a4</td>\n",
       "      <td>9499fb4de537af195d16d046b79fd20a</td>\n",
       "      <td>Mount Holly</td>\n",
       "      <td>VT</td>\n",
       "      <td>Rutland</td>\n",
       "      <td>5758</td>\n",
       "      <td>43.43391</td>\n",
       "      <td>-72.78734</td>\n",
       "      <td>...</td>\n",
       "      <td>159.979400</td>\n",
       "      <td>6511.252601</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>D861732</td>\n",
       "      <td>6e96b921-0c09-4993-bbda-a1ac6411061a</td>\n",
       "      <td>c09a841117fa81b5c8e19afec2760104</td>\n",
       "      <td>Clarksville</td>\n",
       "      <td>TN</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>37042</td>\n",
       "      <td>36.56907</td>\n",
       "      <td>-87.41694</td>\n",
       "      <td>...</td>\n",
       "      <td>207.481100</td>\n",
       "      <td>5695.951810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>I243405</td>\n",
       "      <td>e8307ddf-9a01-4fff-bc59-4742e03fd24f</td>\n",
       "      <td>9c41f212d1e04dca84445019bbc9b41c</td>\n",
       "      <td>Mobeetie</td>\n",
       "      <td>TX</td>\n",
       "      <td>Wheeler</td>\n",
       "      <td>79061</td>\n",
       "      <td>35.52039</td>\n",
       "      <td>-100.44180</td>\n",
       "      <td>...</td>\n",
       "      <td>169.974100</td>\n",
       "      <td>4159.305799</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>I641617</td>\n",
       "      <td>3775ccfc-0052-4107-81ae-9657f81ecdf3</td>\n",
       "      <td>3e1f269b40c235a1038863ecf6b7a0df</td>\n",
       "      <td>Carrollton</td>\n",
       "      <td>GA</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>30117</td>\n",
       "      <td>33.58016</td>\n",
       "      <td>-85.13241</td>\n",
       "      <td>...</td>\n",
       "      <td>252.624000</td>\n",
       "      <td>6468.456752</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>10000</td>\n",
       "      <td>T38070</td>\n",
       "      <td>9de5fb6e-bd33-4995-aec8-f01d0172a499</td>\n",
       "      <td>0ea683a03a3cd544aefe8388aab16176</td>\n",
       "      <td>Clarkesville</td>\n",
       "      <td>GA</td>\n",
       "      <td>Habersham</td>\n",
       "      <td>30523</td>\n",
       "      <td>34.70783</td>\n",
       "      <td>-83.53648</td>\n",
       "      <td>...</td>\n",
       "      <td>217.484000</td>\n",
       "      <td>5857.586167</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CaseOrder Customer_id                           Interaction  \\\n",
       "1              1     K409198  aa90260b-4141-4a24-8e36-b04ce1f4f77b   \n",
       "2              2     S120509  fb76459f-c047-4a9d-8af9-e0f7d4ac2524   \n",
       "3              3     K191035  344d114c-3736-4be5-98f7-c72c281e2d35   \n",
       "4              4      D90850  abfa2b40-2d43-4994-b15a-989b8c79e311   \n",
       "5              5     K662701  68a861fd-0d20-4e51-a587-8a90407ee574   \n",
       "...          ...         ...                                   ...   \n",
       "9996        9996     M324793  45deb5a2-ae04-4518-bf0b-c82db8dbe4a4   \n",
       "9997        9997     D861732  6e96b921-0c09-4993-bbda-a1ac6411061a   \n",
       "9998        9998     I243405  e8307ddf-9a01-4fff-bc59-4742e03fd24f   \n",
       "9999        9999     I641617  3775ccfc-0052-4107-81ae-9657f81ecdf3   \n",
       "10000      10000      T38070  9de5fb6e-bd33-4995-aec8-f01d0172a499   \n",
       "\n",
       "                                    UID          City State  \\\n",
       "1      e885b299883d4f9fb18e39c75155d990   Point Baker    AK   \n",
       "2      f2de8bef964785f41a2959829830fb8a   West Branch    MI   \n",
       "3      f1784cfa9f6d92ae816197eb175d3c71       Yamhill    OR   \n",
       "4      dc8a365077241bb5cd5ccd305136b05e       Del Mar    CA   \n",
       "5      aabb64a116e83fdc4befc1fbab1663f9     Needville    TX   \n",
       "...                                 ...           ...   ...   \n",
       "9996   9499fb4de537af195d16d046b79fd20a   Mount Holly    VT   \n",
       "9997   c09a841117fa81b5c8e19afec2760104   Clarksville    TN   \n",
       "9998   9c41f212d1e04dca84445019bbc9b41c      Mobeetie    TX   \n",
       "9999   3e1f269b40c235a1038863ecf6b7a0df    Carrollton    GA   \n",
       "10000  0ea683a03a3cd544aefe8388aab16176  Clarkesville    GA   \n",
       "\n",
       "                      County    Zip       Lat        Lng  ...  MonthlyCharge  \\\n",
       "1      Prince of Wales-Hyder  99927  56.25100 -133.37571  ...     172.455519   \n",
       "2                     Ogemaw  48661  44.32893  -84.24080  ...     242.632554   \n",
       "3                    Yamhill  97148  45.35589 -123.24657  ...     159.947583   \n",
       "4                  San Diego  92014  32.96687 -117.24798  ...     119.956840   \n",
       "5                  Fort Bend  77461  29.38012  -95.80673  ...     149.948316   \n",
       "...                      ...    ...       ...        ...  ...            ...   \n",
       "9996                 Rutland   5758  43.43391  -72.78734  ...     159.979400   \n",
       "9997              Montgomery  37042  36.56907  -87.41694  ...     207.481100   \n",
       "9998                 Wheeler  79061  35.52039 -100.44180  ...     169.974100   \n",
       "9999                 Carroll  30117  33.58016  -85.13241  ...     252.624000   \n",
       "10000              Habersham  30523  34.70783  -83.53648  ...     217.484000   \n",
       "\n",
       "      Bandwidth_GB_Year Item1 Item2  Item3  Item4  Item5 Item6 Item7 Item8  \n",
       "1            904.536110     5     5      5      3      4     4     3     4  \n",
       "2            800.982766     3     4      3      3      4     3     4     4  \n",
       "3           2054.706961     4     4      2      4      4     3     3     3  \n",
       "4           2164.579412     4     4      4      2      5     4     3     3  \n",
       "5            271.493436     4     4      4      3      4     4     4     5  \n",
       "...                 ...   ...   ...    ...    ...    ...   ...   ...   ...  \n",
       "9996        6511.252601     3     2      3      3      4     3     2     3  \n",
       "9997        5695.951810     4     5      5      4      4     5     2     5  \n",
       "9998        4159.305799     4     4      4      4      4     4     4     5  \n",
       "9999        6468.456752     4     4      6      4      3     3     5     4  \n",
       "10000       5857.586167     2     2      3      3      3     3     4     1  \n",
       "\n",
       "[10000 rows x 50 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "class text:\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "df = pd.read_csv('churn_clean.csv', keep_default_na = False, \n",
    "                na_values = [' ', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN',\n",
    "                            '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "# aligns the df index to match the index of CaseOrder so df[0] doesn't exist\n",
    "df.index = df.index + 1\n",
    "\n",
    "# region Prepare Data\n",
    "# Duplicates\n",
    "def detect_duplicates(col_name):\n",
    "   \"\"\"Detect Duplicates: \n",
    "      Detects duplicate values in a single column.\n",
    "   \"\"\"\n",
    "   print(f\"{col_name}: \",  df.duplicated(subset = col_name).sum())\n",
    "print(text.UNDERLINE + \"Detecting Duplicates\" + text.END)\n",
    "print(\"Exact row: \", len(df)-len(df.drop_duplicates()))\n",
    "detect_duplicates('CaseOrder');\n",
    "detect_duplicates('Customer_id');\n",
    "detect_duplicates('Interaction');\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a1fa",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<p> &nbsp; &nbsp; The ‘is_na()’ function, when used with the ‘sum()’ function will display a count of\n",
    "how many cells contain empty or null values. This operation resulted in no missing values\n",
    "present.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43e0682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\n",
      "Detecting Missing Values\u001b[0m\n",
      "CaseOrder               0\n",
      "Customer_id             0\n",
      "Interaction             0\n",
      "UID                     0\n",
      "City                    0\n",
      "State                   0\n",
      "County                  0\n",
      "Zip                     0\n",
      "Lat                     0\n",
      "Lng                     0\n",
      "Population              0\n",
      "Area                    0\n",
      "TimeZone                0\n",
      "Job                     0\n",
      "Children                0\n",
      "Age                     0\n",
      "Income                  0\n",
      "Marital                 0\n",
      "Gender                  0\n",
      "Churn                   0\n",
      "Outage_sec_perweek      0\n",
      "Email                   0\n",
      "Contacts                0\n",
      "Yearly_equip_failure    0\n",
      "Techie                  0\n",
      "Contract                0\n",
      "Port_modem              0\n",
      "Tablet                  0\n",
      "InternetService         0\n",
      "Phone                   0\n",
      "Multiple                0\n",
      "OnlineSecurity          0\n",
      "OnlineBackup            0\n",
      "DeviceProtection        0\n",
      "TechSupport             0\n",
      "StreamingTV             0\n",
      "StreamingMovies         0\n",
      "PaperlessBilling        0\n",
      "PaymentMethod           0\n",
      "Tenure                  0\n",
      "MonthlyCharge           0\n",
      "Bandwidth_GB_Year       0\n",
      "Item1                   0\n",
      "Item2                   0\n",
      "Item3                   0\n",
      "Item4                   0\n",
      "Item5                   0\n",
      "Item6                   0\n",
      "Item7                   0\n",
      "Item8                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "print(text.UNDERLINE + \"\\nDetecting Missing Values\" + text.END)\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227a453",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<p> &nbsp; &nbsp; To discover outliers, I first sorted numeric values by data source (census values,\n",
    "sign-up values, and business generated values) to add or remove credence to potentially\n",
    "extreme numbers. With the values sorted, the ‘describe()’ function was applied to display\n",
    "the mean, standard deviation, and values at specific percentages of data (quintiles) for\n",
    "every numeric variable. Using my best judgement in observing these results, I have\n",
    "determined that only the ‘Income’ variable contains outliers as the minimum value is\n",
    "348.67 and the maximum value is 258,900.70. Outliers were counted and detected with\n",
    "the interquartile range method (threshold of 1.5) resulting in 336 rows containing outliers\n",
    "for income. As this accounts for only three percent of all rows in the dataset, these rows\n",
    "were removed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c3e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\n",
      "Detecting Outliers\u001b[0m\n",
      "Census Values\n",
      "          Population\n",
      "count   10000.000000\n",
      "mean     9756.562400\n",
      "std     14432.698671\n",
      "min         0.000000\n",
      "25%       738.000000\n",
      "50%      2910.500000\n",
      "75%     13168.000000\n",
      "max    111850.000000\n",
      "\n",
      "Sign-up Values\n",
      "                Age    Children         Income\n",
      "count  10000.000000  10000.0000   10000.000000\n",
      "mean      53.078400      2.0877   39806.926771\n",
      "std       20.698882      2.1472   28199.916702\n",
      "min       18.000000      0.0000     348.670000\n",
      "25%       35.000000      0.0000   19224.717500\n",
      "50%       53.000000      1.0000   33170.605000\n",
      "75%       71.000000      3.0000   53246.170000\n",
      "max       89.000000     10.0000  258900.700000\n",
      "\n",
      "Business Generated Values\n",
      "       Outage_sec_perweek         Email      Contacts  Yearly_equip_failure  \\\n",
      "count        10000.000000  10000.000000  10000.000000          10000.000000   \n",
      "mean            10.001848     12.016000      0.994200              0.398000   \n",
      "std              2.976019      3.025898      0.988466              0.635953   \n",
      "min              0.099747      1.000000      0.000000              0.000000   \n",
      "25%              8.018214     10.000000      0.000000              0.000000   \n",
      "50%             10.018560     12.000000      1.000000              0.000000   \n",
      "75%             11.969485     14.000000      2.000000              1.000000   \n",
      "max             21.207230     23.000000      7.000000              6.000000   \n",
      "\n",
      "             Tenure  MonthlyCharge  Bandwidth_GB_Year  \n",
      "count  10000.000000   10000.000000       10000.000000  \n",
      "mean      34.526188     172.624816        3392.341550  \n",
      "std       26.443063      42.943094        2185.294852  \n",
      "min        1.000259      79.978860         155.506715  \n",
      "25%        7.917694     139.979239        1236.470827  \n",
      "50%       35.430507     167.484700        3279.536903  \n",
      "75%       61.479795     200.734725        5586.141370  \n",
      "max       71.999280     290.160419        7158.981530  \n",
      "\n",
      "\u001b[4mTreating Outliers\u001b[0m\n",
      "Outlier Count for Income: 336\n",
      "New dataframe length: 9664\n",
      "\n",
      "count      9664.000000\n",
      "mean      36735.908054\n",
      "std       22884.354650\n",
      "min         348.670000\n",
      "25%       18759.700000\n",
      "50%       32086.370000\n",
      "75%       50636.515000\n",
      "max      104166.700000\n",
      "Name: Income, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def describe_vars(list_name, list):\n",
    "   \"\"\"Describe Vars: \n",
    "      Used to describe variables and to visually check for outliers. \n",
    "\n",
    "\n",
    "      Compare min and max to each other.\n",
    "      Compare if 25% is close to min and if 75% is close to max.\n",
    "      Consider where the information was obtained.\n",
    "   \"\"\"\n",
    "   print(f\"{list_name} Values\")\n",
    "   print(str(df[list].describe()) + \"\\n\")\n",
    "def treat_outliers(col):\n",
    "   \"\"\"Treat Outliers: \n",
    "      Used to treat outliers using IQR. \n",
    "   \"\"\"\n",
    "   q1 = df[col].quantile(0.25)\n",
    "   q3 = df[col].quantile(0.75)\n",
    "   iqr = q3 - q1\n",
    "   outliers = df[(df[col] < q1 - 1.5 * iqr) | (df[col] > q3 + 1.5 * iqr)]\n",
    "   print(f\"Outlier Count for {col}: {len(outliers)}\")\n",
    "   df.drop(outliers.index, inplace=True)\n",
    "   print(f\"New dataframe length: {len(df)}\\n\" )\n",
    "   print(str(df[col].describe()))\n",
    "   \n",
    "# Outliers\n",
    "census_list = ['Population']\n",
    "signup_list = ['Age', 'Children', 'Income']\n",
    "business_list = ['Outage_sec_perweek', 'Email', 'Contacts', 'Yearly_equip_failure', \n",
    "                'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']\n",
    "print(text.UNDERLINE + \"\\nDetecting Outliers\" + text.END)\n",
    "describe_vars(\"Census\", census_list);\n",
    "describe_vars(\"Sign-up\", signup_list);\n",
    "describe_vars(\"Business Generated\", business_list);\n",
    "print(text.UNDERLINE + \"Treating Outliers\" + text.END)\n",
    "treat_outliers('Income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66177ba2",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "<p> &nbsp; &nbsp; With the cleaning section of data preparation complete, the transformations will\n",
    "now be applied. The ‘filter()’ function retains only the variables described in C2 as these will\n",
    "be used in the decision tree. Next, the ‘get_dummies()’ is applied to all categorical variables\n",
    "to convert them into binary values. To eliminate redundancy for categorical columns that\n",
    "are already binary, the generated ‘No’ columns were dropped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ec1b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\n",
      "Tranforming Data\u001b[0m\n",
      "Excess Columns Removed. Remaining Columns:\n",
      "['Population' 'Children' 'Age' 'Income' 'Outage_sec_perweek'\n",
      " 'Yearly_equip_failure' 'Tenure' 'Bandwidth_GB_Year' 'Area' 'Gender'\n",
      " 'Techie' 'InternetService' 'Multiple' 'DeviceProtection' 'TechSupport']\n",
      "\n",
      "With Dummy Columns:\n",
      "['Population' 'Children' 'Age' 'Income' 'Outage_sec_perweek'\n",
      " 'Yearly_equip_failure' 'Tenure' 'Bandwidth_GB_Year' 'Area_Rural'\n",
      " 'Area_Suburban' 'Area_Urban' 'Gender_Female' 'Gender_Male'\n",
      " 'Gender_Nonbinary' 'Techie_Yes' 'InternetService_DSL'\n",
      " 'InternetService_Fiber Optic' 'InternetService_None' 'Multiple_Yes'\n",
      " 'DeviceProtection_Yes' 'TechSupport_Yes']\n"
     ]
    }
   ],
   "source": [
    "# Data Transformations\n",
    "print(text.UNDERLINE + \"\\nTranforming Data\" + text.END)\n",
    "# filter columns\n",
    "numeric_columns = ['Population', 'Children', 'Age', 'Income', 'Outage_sec_perweek', 'Yearly_equip_failure', 'Tenure', 'Bandwidth_GB_Year']\n",
    "categorical_columns = ['Area', 'Gender','Techie', 'InternetService', 'Multiple', 'DeviceProtection', 'TechSupport']\n",
    "all_columns = numeric_columns + categorical_columns\n",
    "prepared_df = df.filter(all_columns)\n",
    "print(\"Excess Columns Removed. Remaining Columns:\")\n",
    "print(prepared_df.columns.values)\n",
    "prepared_df = pd.get_dummies(prepared_df, dtype=int)\n",
    "# Keeps only one column for binary variables\n",
    "prepared_df.drop(['Techie_No', 'Multiple_No', 'DeviceProtection_No', 'TechSupport_No'], inplace=True, axis=1)\n",
    "print(\"\\nWith Dummy Columns:\")\n",
    "print(prepared_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4faaf",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "<h4>4. Cleaned Data Set</h4>\n",
    "<p> &nbsp; &nbsp; The data preparation results are included in the ‘churn_prepared.csv’ file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "076c2ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformations complete. File saved as 'churn_prepared.csv'\n"
     ]
    }
   ],
   "source": [
    "prepared_df.to_csv('churn_prepared.csv', index=False)\n",
    "print(\"\\nTransformations complete. File saved as 'churn_prepared.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88089071",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<h2>Part IV: Analysis</h2>\n",
    "<h3>D. Data Analysis</h3>\n",
    "<h4>1. Splitting the Data</h4>\n",
    "<p> &nbsp; &nbsp; 80% of the data was split into a training set and 20% into a testing set. These values\n",
    "are found in the following files: ‘Xtrain.csv’, ‘Xtest.csv’, ‘ytrain.csv’, and ‘ytest.csv’. In this\n",
    "case, ‘X’ represents the independent variables and ‘y’ represents the dependent variable\n",
    "(Yearly_equip_failure).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d32541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = prepared_df.drop(['Yearly_equip_failure'], axis=1)\n",
    "y = prepared_df[['Yearly_equip_failure']]\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "             X, y, test_size=0.2, random_state=1001)\n",
    "X_train.to_csv('Xtrain.csv', index=False)\n",
    "X_test.to_csv('Xtest.csv', index= False)\n",
    "y_train.to_csv('ytrain.csv', index= False)\n",
    "y_test.to_csv('ytest.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a4486",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "<h4>2. Output and Intermediate Calculations</h4>\n",
    "<p> &nbsp; &nbsp; A decision tree algorithm was used to analyze the dataset. This process works by\n",
    "splitting the data, like a flowchart, based on the X variables values, then determining the\n",
    "approximate value of the y variable based on training values on this branch of the flow\n",
    "chart. Parameters that control the generation of the decision tree were tested with the\n",
    "‘GridSearchCV()’ function which can be used to determine the optimal depth of a decision\n",
    "tree or determine how a node on the tree should be split. The decision tree features I tested\n",
    "were the splitting methodology (‘criterion’), splitting strategy (‘splitter’), depth of the tree\n",
    "(‘max_depth’), sample count required for splitting (‘min_samples_split’), and the maximum\n",
    "number of features to consider for a node split (‘max_features’).\n",
    "\n",
    " &nbsp; &nbsp; The resulting test determined the best tree used entropy (which is the measure of\n",
    "randomness) for the node criterion, selected a random feature for the split, had a max\n",
    "depth of five, with a seven minimum samples requirement for splitting and considered\n",
    "three features for a node split. No intermediate calculations were necessary.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e184fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 3, 'min_samples_split': 7, 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "# Tunning Tree [In-Text Citation: (Navlani, 2024)]\n",
    "dt = DecisionTreeClassifier(random_state=1001)\n",
    "# creating testing parameters [In-Text Citation: (scikit-learn, n.d.)]\n",
    "testing_params = {\n",
    "   \"criterion\": ['gini', 'entropy'], # methodology for determining split\n",
    "   \"splitter\": ['best', 'random'], # strategy for splitting\n",
    "   \"max_depth\": [2, 3, 4, 5, 6, 7, 8, None], # how deep the tree is allowed to be\n",
    "   \"min_samples_split\": [2, 3, 4, 5, 6, 7, 8], # minimum count of samples to make a node split\n",
    "   \"max_features\": [None, 1, 2, 3, 4, 5, 'sqrt', 'log2'] # features to consider when picking best split\n",
    "}\n",
    "# Hyper parameter tunning [In-Text Citation: (Saini, 2020)]\n",
    "# With GridSearch [In-Text Citation: (scikit-learn, n.d.-b)]\n",
    "# Using KFold [In-Text Citation: (MAdness & EEtch, 2022)]\n",
    "tunning = GridSearchCV(dt, param_grid=testing_params, n_jobs=-1, cv=KFold(5))\n",
    "tunning.fit(X_train, y_train)\n",
    "print(f\"Best Decision Tree Parameters: {tunning.best_params_}\")\n",
    "\n",
    "\"\"\"While I can just use the 'tunning' variable for prediction, I created another tree with \n",
    "the best parameters so I don't have to run a gridsearch, which takes a while, every time\n",
    "\"\"\"\n",
    "better_dt = DecisionTreeClassifier(\n",
    "   random_state=1001,\n",
    "   criterion = 'entropy',\n",
    "   max_depth = 5,\n",
    "   max_features = 3, \n",
    "   min_samples_split = 7, \n",
    "   splitter = 'random'\n",
    "   )\n",
    "better_dt.fit(X_train, y_train)\n",
    "y_pred = better_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f66f6",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<h2>Part V: Data Summary and Implications</h2>\n",
    "<h3>E. Data Analysis Summary</h3>\n",
    "<h4>1. Accuracy and MSE</h4>\n",
    "<p> &nbsp; &nbsp; The model correctly predicted 67.0% of all test values for the y variable. While this\n",
    "may seem like a decent accuracy for predicting variables, the distribution of values for\n",
    "‘Yearly_equip_failure’ (pictured below) indicate a bias towards ‘0’ with 67.3% of all values\n",
    "being this. The mean squared error(MSE) of tree rounded is 0.570 and the root mean\n",
    "squared error(RMSE) is about 0.755. And again, like with the accuracy percentage, while\n",
    "these values can indicate a greater model when they are smaller numbers, the range of the\n",
    "dependent variable goes from one to six. So in this scenario these metrics are high error\n",
    "scores. The final form of measurement derived from the analysis is the R-Squared value\n",
    "which was −0.399. This metric indicates how much variance is accounted for in a line of\n",
    "best fit where “the greater the value of R-Squared, the better the regression model” (Kumar,\n",
    "2023). Since this value is negative, the model is very poor at predicting values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7162bcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\n",
      "Y Variable Distribution\u001b[0m\n",
      "                      count percentage\n",
      "Yearly_equip_failure                  \n",
      "0                      6505      67.3%\n",
      "1                      2578      26.7%\n",
      "2                       493       5.1%\n",
      "3                        80       0.8%\n",
      "4                         7       0.1%\n",
      "6                         1       0.0%\n"
     ]
    }
   ],
   "source": [
    "# show distribution of y variable [In-Text Citation: (stats writer, 2023)]\n",
    "print(text.UNDERLINE + \"\\nY Variable Distribution\" + text.END)\n",
    "counts = prepared_df['Yearly_equip_failure'].value_counts()\n",
    "percent = prepared_df['Yearly_equip_failure'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "print(pd.concat([counts,percent], axis=1, keys=['count', 'percentage']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a467b",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<h4>2. Results and Implications</h4>\n",
    "<p> &nbsp; &nbsp; All model metrics demonstrate that this decision tree cannot be trusted to\n",
    "consistently predict how many annual equipment failures a customer will have. The RSME\n",
    "can be interpreted as “measure[ing] the standard deviation of residuals” so having it be as\n",
    "high as 0.755 when 94% of data falls between ‘0’ and ‘1’ shows a clear fault in this machine\n",
    "learning algorithm (Chugh, 2020). Taking a look at the actual predicted values, every value\n",
    "was predicted as zero.\n",
    "\n",
    " &nbsp; &nbsp; Even with the hyperparameter tuning performed, it cannot overcome the heavy favor\n",
    "towards zero. This could imply that equipment failure cannot easily be predicted, greater\n",
    "hyperparameter tuning should be performed, or perhaps a different machine learning\n",
    "algorithm would be more suitable for this variable.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a3a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy : 0.6704604242110709, (67.0 %)\n",
      "Mean Squared Error: 0.5695809622348681\n",
      "Root Mean Squared Error: 0.7547058779649647\n",
      "R Squared: -0.398872217022306\n",
      "\u001b[4m\n",
      "Prediction Distributions\u001b[0m\n",
      "Yearly_equip_failure_predict\n",
      "0    100.0%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Prediction Accuracy : {accuracy}, ({round(accuracy * 100, 1)} %)\")\n",
    "# MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "# RMSE\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "# R Squared\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"R Squared: {r_squared}\")\n",
    "# Predicted values\n",
    "print(text.UNDERLINE + \"\\nPrediction Distributions\" + text.END)\n",
    "prediction_df = pd.DataFrame(y_pred, columns=['Yearly_equip_failure_predict'])\n",
    "print(prediction_df['Yearly_equip_failure_predict'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37c14b",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    h1,h2,h3,h4{\n",
    "        margin-bottom: 0px;\n",
    "    }\n",
    "    p,ul{\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    h2{\n",
    "        margin-left: 10px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    h3{\n",
    "        margin-left: 20px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    h4{\n",
    "        margin-left: 30px;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "    p{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 7px;\n",
    "    }\n",
    "    li{\n",
    "        margin-left: 40px;\n",
    "        margin-top: 5px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<h4>3. Limitation</h4>\n",
    "<p> &nbsp; &nbsp; An important limitation to understand when it comes to the decision tree algorithm\n",
    "itself, is its greedy approach to creating nodes on the tree. The method selects the best\n",
    "feature to act as a node based on minimal impurity. The more homogenous values are for a\n",
    "variable, the less impurities it has which increases its chances of being a node. This\n",
    "selection process is called greedy because it only “does the best split at a given step”\n",
    "without considering if this would be the best split for the overall formation of the tree\n",
    "(Dhote, 2020). It would take a lengthy amount of time to create endless combinations of\n",
    "nodes and conditions that could lead to better predictive results. A decision tree model\n",
    "only calculates with the values in front of it, which sometimes misses the overall scope of\n",
    "the data.</p>\n",
    "<h4>4. Course of Action</h4>\n",
    "<p> &nbsp; &nbsp; The conclusion posed by the MSE, RMSE, and R-Squared scores indicate this\n",
    "decision tree is inadequate at calculating a customer’s equipment failure. With this in\n",
    "mind, I recommend the data analytics team conduct further testing on the data. There are\n",
    "various factors that, once tweaked, could lead to a better model. Some examples include\n",
    "greater hyperparameter tunning, ensuring equal distributions of y values persists in the\n",
    "training dataset, or reevaluating the selection of independent variables. Beyond that,\n",
    "considering a random forest model or another form of advanced regression could also\n",
    "improve predictiveness. In relation to the question posed at the beginning of this project, a\n",
    "customer’s information cannot be used to discover the likeliness of failing equipment, but\n",
    "further testing could change that answer.</p>\n",
    "\n",
    "<h3>G. Sources For Third-Party Code</h3>\n",
    "<p>MAdness & EEtch. (2022, November 15). UserWarning: The least populated class in y has\n",
    "only 1 members, which is less than n_splits=5. Stack Overflow.\n",
    "<a href=\"https://stackoverflow.com/questions/74445334/userwarning-the-least-populated-class-in-y-has-only-1-members-which-is-less-th\">https://stackoverflow.com/questions/74445334/userwarning-the-least-populated-class-in-y-has-only-1-members-which-is-less-th</a>\n",
    "\n",
    "Navlani, A. (2024, June 27). Decision Tree Classification in Python Tutorial. DataCamp.\n",
    "https://www.datacamp.com/tutorial/decision-tree-classification-python\n",
    "\n",
    "Saini, B. (2020, September 29). Hyperparameter Tuning of Decision Tree Classifier Using\n",
    "GridSearchCV. plainenglish.io/blog/hyperparameter-tuning-of-decision-tree-\n",
    "classifier-using-gridsearchcv-2a6ebcaffeda.\n",
    "https://plainenglish.io/blog/hyperparameter-tuning-of-decision-tree-classifier-using-gridsearchcv-2a6ebcaffeda\n",
    "\n",
    "scikit-learn. (n.d.). DecisionTreeClassifier. Scikit-learn. https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "scikit-learn. (n.d.-b). GridSearchCV. Scikit-learn. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "stats writer. (2023, November 1). How to represent value_counts as Percentage in Pandas?\n",
    "PSYCHOLOGICAL SCALES. https://scales.arabpsychology.com/stats/how-to-represent-value_counts-as-percentage-in-pandas/\n",
    "</p>\n",
    "\n",
    "<h4>H. Sources</h4>\n",
    "<p>Chugh, A. (2020, December 7). MAE, MSE, RMSE, Coefficient of Determination, Adjusted R\n",
    "Squared — Which Metric is Better? Medium. <a href=\"https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e\">https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e</a>\n",
    "\n",
    "Dhote, P. (2020, July 26). Decision Tree Explained: Classification and Regression\n",
    "Algorithm. Medium. https://pradeep-dhote9.medium.com/decision-tree-explained-classification-and-regression-algorithm-d378010406fe\n",
    "\n",
    "Filho, M. (2023, March 24). Do Decision Trees Need Feature Scaling Or Normalization?\n",
    "Forecastegy. https://forecastegy.com/posts/do-decision-trees-need-feature-scaling-or-normalization/\n",
    "\n",
    "Kumar, A. (2023, December 29). Mean Squared Error or R-Squared - Which one to use?\n",
    "Analytics Yogi. https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/\n",
    "\n",
    "Ttcbest. (2023, October 8). Decision Tree (assumptions, advantages, disadvantages,\n",
    "applications). Medium. https://medium.com/@ttc4best/decision-tree-assumptions-advantages-disadvantages-applications-46f7e5807517\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
